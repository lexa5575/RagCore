#!/usr/bin/env python3
"""
Universal Document Indexer for RAG Systems
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö RAG 2024:
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (Delta indexing)
- –ú–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ (Multi-vector indexing)
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (Contextual chunk enrichment)
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ (Semantic chunking)
- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Document versioning)
- –§—Ä–µ–π–º–≤–æ—Ä–∫-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (Framework agnostic)
"""

import os
import sys
import logging
import yaml
import hashlib
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum

import chromadb
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.text_splitter import get_text_splitter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IndexingMode(Enum):
    """–†–µ–∂–∏–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    FULL = "full"           # –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    INCREMENTAL = "incremental"  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    DELTA = "delta"         # –¢–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    FRAMEWORK_ONLY = "framework_only"  # –¢–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫

class DocumentType(Enum):
    """–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    MARKDOWN = "markdown"
    HTML = "html"
    TEXT = "text"
    VITEPRESS = "vitepress"
    DOCUSAURUS = "docusaurus"
    GITBOOK = "gitbook"

@dataclass
class DocumentMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
    file_path: str
    framework: str
    document_type: DocumentType
    file_hash: str
    last_modified: str
    created_at: str
    file_size: int
    version: int = 1
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        data = asdict(self)
        data['document_type'] = self.document_type.value
        return data

@dataclass
class ChunkMetadata:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞"""
    chunk_id: str
    document_id: str
    framework: str
    source_file: str
    section_title: str
    chunk_index: int
    chunk_type: str
    context_window: str
    parent_sections: List[str]
    semantic_labels: List[str]
    confidence_score: float
    created_at: str
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return asdict(self)

class UniversalDocumentIndexer:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –í—Å–µ —Ç–∏–ø—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –∏–∑ config.yaml
    - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    """
    
    def __init__(self, config_path: str = 'config.yaml'):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞"""
        self.config = self._load_config(config_path)
        self.document_metadata_cache = {}
        self.processed_documents = set()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        self.client = chromadb.PersistentClient(
            path=self.config['database']['path']
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ embeddings
        self.embedder = SentenceTransformer(
            self.config['embeddings']['model']
        )
        
        # –ö—ç—à –¥–ª—è text_splitter'–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
        self.text_splitters = {}
        
        logger.info("‚úÖ Universal Document Indexer initialized")
        
    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            required_sections = ['database', 'embeddings', 'frameworks']
            for section in required_sections:
                if section not in config:
                    raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–µ–∫—Ü–∏—è {section} –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                    
            return config
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    def _get_text_splitter(self, framework: str):
        """–ü–æ–ª—É—á–∞–µ—Ç text_splitter –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        if framework not in self.text_splitters:
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            chunk_sizes = {
                'laravel': 800,
                'vue': 600,
                'filament': 700,
                'alpine': 500,
                'inertia': 600,
                'tailwindcss': 600
            }
            
            overlap_sizes = {
                'laravel': 150,
                'vue': 100,
                'filament': 120,
                'alpine': 80,
                'inertia': 100,
                'tailwindcss': 100
            }
            
            chunk_size = chunk_sizes.get(framework, 700)
            overlap_size = overlap_sizes.get(framework, 120)
            
            self.text_splitters[framework] = get_text_splitter(
                framework=framework,
                chunk_size=chunk_size,
                overlap_size=overlap_size
            )
            
        return self.text_splitters[framework]
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        try:
            with open(file_path, 'rb') as f:
                file_hash = hashlib.sha256(f.read()).hexdigest()
            return file_hash
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à –¥–ª—è {file_path}: {e}")
            return ""
    
    def _get_document_metadata(self, file_path: Path, framework: str) -> DocumentMetadata:
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        file_stat = file_path.stat()
        file_hash = self._calculate_file_hash(file_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        framework_config = self.config['frameworks'].get(framework, {})
        doc_type = DocumentType(framework_config.get('type', 'markdown'))
        
        return DocumentMetadata(
            file_path=str(file_path),
            framework=framework,
            document_type=doc_type,
            file_hash=file_hash,
            last_modified=datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
            created_at=datetime.now().isoformat(),
            file_size=file_stat.st_size
        )
    
    def _should_process_document(self, file_path: Path, framework: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)"""
        current_hash = self._calculate_file_hash(file_path)
        doc_key = f"{framework}:{str(file_path)}"
        
        if doc_key in self.document_metadata_cache:
            cached_hash = self.document_metadata_cache[doc_key].get('file_hash')
            if cached_hash == current_hash:
                logger.debug(f"–î–æ–∫—É–º–µ–Ω—Ç {file_path.name} –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
        
        return True
    
    def _find_framework_documents(self, framework: str) -> List[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        framework_config = self.config['frameworks'].get(framework, {})
        if not framework_config.get('enabled', True):
            logger.info(f"–§—Ä–µ–π–º–≤–æ—Ä–∫ {framework} –æ—Ç–∫–ª—é—á–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return []
        
        docs_path = Path(framework_config['path'])
        if not docs_path.exists():
            logger.warning(f"–ü–∞–ø–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {docs_path}")
            return []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        doc_type = framework_config.get('type', 'markdown')
        
        extensions = {
            'markdown': ['*.md', '*.markdown'],
            'vitepress': ['*.md', '*.vue'],
            'docusaurus': ['*.md', '*.mdx'],
            'gitbook': ['*.md'],
            'html': ['*.html', '*.htm'],
            'text': ['*.txt']
        }
        
        files = []
        for ext in extensions.get(doc_type, ['*.md']):
            files.extend(docs_path.rglob(ext))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ exclude_patterns
        exclude_patterns = self.config.get('auto_scan', {}).get('exclude_patterns', [])
        filtered_files = []
        
        for file_path in files:
            should_exclude = False
            for pattern in exclude_patterns:
                if pattern in str(file_path):
                    should_exclude = True
                    break
            
            if not should_exclude:
                filtered_files.append(file_path)
        
        return filtered_files
    
    def _process_document(self, file_path: Path, framework: str) -> List[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                logger.warning(f"–§–∞–π–ª {file_path.name} –ø—É—Å—Ç–æ–π")
                return []
            
            # –ü–æ–ª—É—á–∞–µ–º text_splitter –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            text_splitter = self._get_text_splitter(framework)
            
            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if hasattr(text_splitter, 'preprocess_markdown'):
                content = text_splitter.preprocess_markdown(content)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_metadata = self._get_document_metadata(file_path, framework)
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
            base_metadata = {
                'document_id': f"{framework}:{file_path.stem}",
                'framework': framework,
                'source_file': file_path.stem,
                'file_path': str(file_path),
                'document_type': doc_metadata.document_type.value,
                'file_hash': doc_metadata.file_hash,
                'last_modified': doc_metadata.last_modified,
                'created_at': doc_metadata.created_at
            }
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = text_splitter.split_text(content, base_metadata)
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            enriched_chunks = self._enrich_chunks_with_context(chunks, content, framework)
            
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω {file_path.name}: {len(enriched_chunks)} —á–∞–Ω–∫–æ–≤")
            
            return enriched_chunks
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []
    
    def _enrich_chunks_with_context(self, chunks: List[Dict], full_content: str, framework: str) -> List[Dict]:
        """
        –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (Context-enriched chunking)
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ–≥–ª–∞—Å–Ω–æ –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º RAG 2024
        """
        enriched_chunks = []
        
        for i, chunk in enumerate(chunks):
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–µ
            enriched_chunk = chunk.copy()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞
            enriched_chunk['metadata']['chunk_index'] = i
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ (window-based context)
            context_window = self._create_context_window(chunks, i, window_size=2)
            enriched_chunk['metadata']['context_window'] = context_window
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏
            semantic_labels = self._generate_semantic_labels(chunk['content'], framework)
            enriched_chunk['metadata']['semantic_labels'] = semantic_labels
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å–µ–∫—Ü–∏—è—Ö
            parent_sections = self._extract_parent_sections(chunk['content'])
            enriched_chunk['metadata']['parent_sections'] = parent_sections
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence_score = self._calculate_confidence_score(chunk['content'])
            enriched_chunk['metadata']['confidence_score'] = confidence_score
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø —á–∞–Ω–∫–∞
            chunk_type = self._classify_chunk_type(chunk['content'])
            enriched_chunk['metadata']['chunk_type'] = chunk_type
            
            enriched_chunks.append(enriched_chunk)
        
        return enriched_chunks
    
    def _create_context_window(self, chunks: List[Dict], current_index: int, window_size: int = 2) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è —á–∞–Ω–∫–∞"""
        context_parts = []
        
        # –ü—Ä–µ–¥—ã–¥—É—â–∏–µ —á–∞–Ω–∫–∏
        start_idx = max(0, current_index - window_size)
        for i in range(start_idx, current_index):
            context_parts.append(f"Previous: {chunks[i]['content'][:100]}...")
        
        # –°–ª–µ–¥—É—é—â–∏–µ —á–∞–Ω–∫–∏
        end_idx = min(len(chunks), current_index + window_size + 1)
        for i in range(current_index + 1, end_idx):
            context_parts.append(f"Next: {chunks[i]['content'][:100]}...")
        
        return " | ".join(context_parts)
    
    def _generate_semantic_labels(self, content: str, framework: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ –¥–ª—è —á–∞–Ω–∫–∞"""
        labels = []
        
        # –§—Ä–µ–π–º–≤–æ—Ä–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç–∫–∏
        framework_keywords = {
            'laravel': ['eloquent', 'blade', 'artisan', 'migration', 'model', 'controller', 'route'],
            'vue': ['component', 'directive', 'reactive', 'composition', 'template', 'props', 'emit'],
            'filament': ['resource', 'form', 'table', 'action', 'widget', 'page', 'relation'],
            'alpine': ['data', 'show', 'if', 'for', 'model', 'click', 'init'],
            'inertia': ['visit', 'form', 'link', 'router', 'props', 'page', 'component'],
            'tailwindcss': ['utility', 'responsive', 'hover', 'focus', 'dark', 'variant', 'class']
        }
        
        content_lower = content.lower()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
        for keyword in framework_keywords.get(framework, []):
            if keyword in content_lower:
                labels.append(keyword)
        
        # –û–±—â–∏–µ –º–µ—Ç–∫–∏
        if 'example' in content_lower or '–∫–æ–¥' in content_lower:
            labels.append('code_example')
        
        if 'api' in content_lower:
            labels.append('api_reference')
        
        if 'tutorial' in content_lower or 'guide' in content_lower:
            labels.append('tutorial')
        
        if 'configuration' in content_lower or 'config' in content_lower:
            labels.append('configuration')
        
        return labels
    
    def _extract_parent_sections(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Å–µ–∫—Ü–∏—è—Ö"""
        sections = []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ markdown
        lines = content.split('\n')
        for line in lines:
            if line.strip().startswith('#'):
                # –£–±–∏—Ä–∞–µ–º # –∏ –ø–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = line.strip().lstrip('#').strip()
                if title:
                    sections.append(title)
        
        return sections[:3]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 —Å–µ–∫—Ü–∏–π
    
    def _calculate_confidence_score(self, content: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —á–∞–Ω–∫–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∞
        score = 0.5  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if 50 <= len(content) <= 1000:
            score += 0.2
        
        # –ù–∞–ª–∏—á–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if '```' in content:  # –ë–ª–æ–∫–∏ –∫–æ–¥–∞
            score += 0.15
        
        if content.count('\n') > 2:  # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            score += 0.1
        
        # –ù–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        if any(marker in content.lower() for marker in ['example', 'note', 'important', 'warning']):
            score += 0.05
        
        return min(1.0, score)
    
    def _classify_chunk_type(self, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —á–∞–Ω–∫–∞"""
        content_lower = content.lower()
        
        if '```' in content:
            return 'code_block'
        elif content_lower.startswith('# '):
            return 'header'
        elif 'example' in content_lower:
            return 'example'
        elif 'note' in content_lower or 'important' in content_lower:
            return 'note'
        elif 'api' in content_lower or 'method' in content_lower:
            return 'api_reference'
        else:
            return 'general'
    
    def _add_chunks_to_collection(self, collection, chunks: List[Dict], framework: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        if not chunks:
            return
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ChromaDB
        documents = []
        metadatas = []
        ids = []
        
        for chunk in chunks:
            documents.append(chunk['content'])
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ —á–∏—Å–ª–∞
            metadata = self._prepare_metadata_for_chromadb(chunk['metadata'])
            metadatas.append(metadata)
            ids.append(chunk['metadata']['chunk_id'])
        
        # –°–æ–∑–¥–∞–µ–º embeddings
        logger.debug(f"–°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(documents)} —á–∞–Ω–∫–æ–≤ {framework}...")
        embeddings = self.embedder.encode(documents, show_progress_bar=False).tolist()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids,
            embeddings=embeddings
        )
    
    def _prepare_metadata_for_chromadb(self, metadata: Dict) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è ChromaDB (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –∏ —á–∏—Å–ª–∞)"""
        prepared = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float)):
                prepared[key] = value
            elif isinstance(value, list):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏
                prepared[key] = ', '.join(str(item) for item in value)
            elif isinstance(value, dict):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ JSON —Å—Ç—Ä–æ–∫–∏
                prepared[key] = json.dumps(value)
            else:
                prepared[key] = str(value)
        
        return prepared
    
    def clear_framework_data(self, framework: str):
        """–û—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        logger.info(f"üóëÔ∏è  –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework}...")
        
        try:
            collection = self.client.get_collection(
                self.config['database']['collection_name']
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            framework_docs = collection.get(
                where={"framework": framework},
                include=['ids']
            )
            
            if framework_docs['ids']:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(framework_docs['ids'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ {framework}")
                collection.delete(ids=framework_docs['ids'])
                logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework} —É–¥–∞–ª–µ–Ω—ã")
            else:
                logger.info(f"–î–∞–Ω–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö {framework}: {e}")
    
    def clear_all_data(self):
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"""
        logger.info("üóëÔ∏è  –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ...")
        
        try:
            # –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.client.delete_collection(
                self.config['database']['collection_name']
            )
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.client.create_collection(
                self.config['database']['collection_name']
            )
            
            logger.info("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def process_framework(self, framework: str, mode: IndexingMode = IndexingMode.FULL):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        logger.info(f"üìö –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ {framework} –≤ —Ä–µ–∂–∏–º–µ {mode.value}")
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
        documents = self._find_framework_documents(framework)
        if not documents:
            logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(documents)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        try:
            collection = self.client.get_collection(
                self.config['database']['collection_name']
            )
        except:
            collection = self.client.create_collection(
                self.config['database']['collection_name']
            )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        total_chunks = 0
        processed_files = 0
        
        for doc_path in tqdm(documents, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {framework}"):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
                if mode == IndexingMode.INCREMENTAL and not self._should_process_document(doc_path, framework):
                    continue
                
                chunks = self._process_document(doc_path, framework)
                if chunks:
                    self._add_chunks_to_collection(collection, chunks, framework)
                    total_chunks += len(chunks)
                    processed_files += 1
                    logger.debug(f"‚úÖ {doc_path.name}: —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
                else:
                    logger.warning(f"‚ö†Ô∏è  {doc_path.name}: —á–∞–Ω–∫–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {doc_path.name}: {e}")
        
        logger.info(f"üéâ –§—Ä–µ–π–º–≤–æ—Ä–∫ {framework} –æ–±—Ä–∞–±–æ—Ç–∞–Ω! –§–∞–π–ª–æ–≤: {processed_files}, —á–∞–Ω–∫–æ–≤: {total_chunks}")
    
    def process_all_frameworks(self, mode: IndexingMode = IndexingMode.FULL):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏"""
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –≤ —Ä–µ–∂–∏–º–µ {mode.value}")
        
        if mode == IndexingMode.FULL:
            self.clear_all_data()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
        active_frameworks = [
            name for name, config in self.config['frameworks'].items()
            if config.get('enabled', True)
        ]
        
        logger.info(f"–ê–∫—Ç–∏–≤–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏: {', '.join(active_frameworks)}")
        
        for framework in active_frameworks:
            try:
                if mode == IndexingMode.FULL:
                    # –ü—Ä–∏ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
                    self.clear_framework_data(framework)
                
                self.process_framework(framework, mode)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ {framework}: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._verify_indexing_results()
    
    def _verify_indexing_results(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        
        try:
            collection = self.client.get_collection(
                self.config['database']['collection_name']
            )
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_docs = collection.count()
            logger.info(f"üìä –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {total_docs}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º
            framework_stats = {}
            for framework in self.config['frameworks'].keys():
                framework_docs = collection.get(
                    where={"framework": framework},
                    include=['metadatas']
                )
                
                if framework_docs['metadatas']:
                    framework_stats[framework] = len(framework_docs['metadatas'])
            
            logger.info("üìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º:")
            for framework, count in sorted(framework_stats.items()):
                logger.info(f"   {framework}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            self._check_data_quality(collection)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
    
    def _check_data_quality(self, collection):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_docs = collection.get(include=['metadatas'])
            
            if not all_docs['metadatas']:
                logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
                return
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_keys = set()
            for metadata in all_docs['metadatas']:
                metadata_keys.update(metadata.keys())
            
            logger.info(f"üìä –ö–ª—é—á–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {', '.join(sorted(metadata_keys))}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã —á–∞–Ω–∫–æ–≤
            chunk_types = {}
            for metadata in all_docs['metadatas']:
                chunk_type = metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            logger.info("üìä –¢–∏–ø—ã —á–∞–Ω–∫–æ–≤:")
            for chunk_type, count in sorted(chunk_types.items()):
                logger.info(f"   {chunk_type}: {count}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence_scores = []
            for metadata in all_docs['metadatas']:
                try:
                    score = float(metadata.get('confidence_score', 0))
                    confidence_scores.append(score)
                except (ValueError, TypeError):
                    pass
            
            if confidence_scores:
                avg_confidence = sum(confidence_scores) / len(confidence_scores)
                logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {avg_confidence:.3f}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def test_search_quality(self, framework: str = None):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞...")
        
        try:
            collection = self.client.get_collection(
                self.config['database']['collection_name']
            )
            
            # –§—Ä–µ–π–º–≤–æ—Ä–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            test_queries = {
                'laravel': [
                    "–ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é –≤ Laravel?",
                    "php artisan make:migration",
                    "Laravel Eloquent –º–æ–¥–µ–ª–∏",
                    "Blade —à–∞–±–ª–æ–Ω—ã",
                    "Laravel routing"
                ],
                'vue': [
                    "Vue composition API",
                    "Vue –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã",
                    "—Ä–µ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ Vue",
                    "Vue –¥–∏—Ä–µ–∫—Ç–∏–≤—ã",
                    "props –∏ emit"
                ],
                'filament': [
                    "Filament —Ä–µ—Å—É—Ä—Å—ã",
                    "Filament —Ñ–æ—Ä–º—ã",
                    "Filament —Ç–∞–±–ª–∏—Ü—ã",
                    "Filament –¥–µ–π—Å—Ç–≤–∏—è",
                    "Filament –≤–∏–¥–∂–µ—Ç—ã"
                ]
            }
            
            # –ï—Å–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–µ —É–∫–∞–∑–∞–Ω, —Ç–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ
            frameworks_to_test = [framework] if framework else list(test_queries.keys())
            
            for test_framework in frameworks_to_test:
                if test_framework not in test_queries:
                    continue
                    
                logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ {test_framework}")
                
                for query in test_queries[test_framework]:
                    self._test_single_query(collection, query, test_framework)
                    
                logger.info("")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")
    
    def _test_single_query(self, collection, query: str, framework: str):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        try:
            # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedder.encode([query]).tolist()
            
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=3,
                where={"framework": framework},
                include=['metadatas', 'documents', 'distances']
            )
            
            if results['documents'][0]:
                logger.info(f"‚úÖ '{query}' -> {len(results['documents'][0])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                for i, (doc, meta, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    relevance = 1 - distance
                    section_title = meta.get('section_title', 'No title')
                    chunk_type = meta.get('chunk_type', 'unknown')
                    
                    logger.info(f"   {i+1}. {section_title} ({chunk_type}) - {relevance:.3f}")
                    logger.info(f"      {doc[:80]}...")
            else:
                logger.warning(f"‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è: '{query}'")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º")
    parser.add_argument('--mode', choices=['full', 'incremental', 'delta'], 
                       default='full', help='–†–µ–∂–∏–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
    parser.add_argument('--framework', type=str, help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫')
    parser.add_argument('--test', action='store_true', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞')
    parser.add_argument('--clear', action='store_true', help='–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ')
    parser.add_argument('--verify', action='store_true', help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    try:
        indexer = UniversalDocumentIndexer()
        
        if args.clear:
            indexer.clear_all_data()
            logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")
            return 0
        
        if args.verify:
            indexer._verify_indexing_results()
            return 0
        
        if args.test:
            indexer.test_search_quality(args.framework)
            return 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        mode = IndexingMode(args.mode)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if args.framework:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫
            indexer.process_framework(args.framework, mode)
        else:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
            indexer.process_all_frameworks(mode)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞
        indexer.test_search_quality(args.framework)
        
        logger.info("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())
