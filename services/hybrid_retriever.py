#!/usr/bin/env python3
"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Dense + Sparse) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ RAG
–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–º –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import logging
import re
from typing import List, Dict, Any, Tuple
from collections import Counter
import math
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class BM25Retriever:
    """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è BM25 –¥–ª—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.documents = []
        self.doc_freqs = []
        self.idf = {}
        self.doc_len = []
        self.avgdl = 0
        
    def fit(self, documents: List[str]):
        """–û–±—É—á–∞–µ—Ç BM25 –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        self.documents = documents
        self.doc_len = [len(doc.split()) for doc in documents]
        self.avgdl = sum(self.doc_len) / len(self.doc_len) if self.doc_len else 0
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        self.doc_freqs = []
        df = Counter()
        
        for doc in documents:
            words = doc.lower().split()
            word_freq = Counter(words)
            self.doc_freqs.append(word_freq)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º document frequency –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
            for word in set(words):
                df[word] += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
        num_docs = len(documents)
        for word, freq in df.items():
            self.idf[word] = math.log((num_docs - freq + 0.5) / (freq + 0.5))
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç BM25 –ø–æ–∏—Å–∫"""
        if not self.documents:
            return []
            
        query_words = query.lower().split()
        scores = []
        
        for doc_idx, doc_freq in enumerate(self.doc_freqs):
            score = 0
            doc_len = self.doc_len[doc_idx]
            
            for word in query_words:
                if word in doc_freq:
                    tf = doc_freq[word]
                    idf = self.idf.get(word, 0)
                    
                    # BM25 —Ñ–æ—Ä–º—É–ª–∞
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
                    score += idf * (numerator / denominator)
            
            scores.append((doc_idx, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]

class HybridRetriever:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    """
    
    def __init__(self, embedder, collection, config: Dict[str, Any]):
        self.embedder = embedder
        self.collection = collection
        self.config = config
        self.bm25_retrievers = {}  # –ö—ç—à BM25 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
        self.framework_documents = {}  # –ö—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º
        
    def _get_or_create_bm25(self, framework: str = None) -> BM25Retriever:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç BM25 —Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        cache_key = framework or "all"
        
        if cache_key not in self.bm25_retrievers:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            where_filter = {"framework": framework} if framework else None
            
            try:
                results = self.collection.get(
                    where=where_filter,
                    include=['documents', 'metadatas']
                )
                
                if results['documents']:
                    documents = results['documents']
                    metadatas = results['metadatas']
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    self.framework_documents[cache_key] = {
                        'documents': documents,
                        'metadatas': metadatas
                    }
                    
                    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º BM25
                    bm25 = BM25Retriever()
                    bm25.fit(documents)
                    self.bm25_retrievers[cache_key] = bm25
                    
                    logger.info(f"–°–æ–∑–¥–∞–Ω BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è {framework or '–≤—Å–µ—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤'}: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                else:
                    logger.warning(f"–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è BM25 –∏–Ω–¥–µ–∫—Å–∞: {framework}")
                    return None
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è BM25 –∏–Ω–¥–µ–∫—Å–∞: {e}")
                return None
        
        return self.bm25_retrievers.get(cache_key)
    
    def _semantic_search(self, query: str, framework: str = None, max_results: int = 10) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB"""
        try:
            query_embedding = self.embedder.encode([query])
            
            where_filter = {"framework": framework} if framework else None
            
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=max_results,
                where=where_filter
            )
            
            documents = []
            if results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    documents.append({
                        'content': doc,
                        'metadata': metadata,
                        'semantic_score': 1 - distance,
                        'rank': i + 1,
                        'source': 'semantic'
                    })
            
            return documents
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _lexical_search(self, query: str, framework: str = None, max_results: int = 10) -> List[Dict]:
        """–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ BM25"""
        try:
            bm25 = self._get_or_create_bm25(framework)
            if not bm25:
                return []
            
            cache_key = framework or "all"
            framework_data = self.framework_documents.get(cache_key, {})
            
            if not framework_data:
                return []
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º BM25 –ø–æ–∏—Å–∫
            bm25_results = bm25.search(query, max_results)
            
            documents = []
            for doc_idx, bm25_score in bm25_results:
                if doc_idx < len(framework_data['documents']):
                    documents.append({
                        'content': framework_data['documents'][doc_idx],
                        'metadata': framework_data['metadatas'][doc_idx],
                        'lexical_score': bm25_score,
                        'rank': len(documents) + 1,
                        'source': 'lexical'
                    })
            
            return documents
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _normalize_scores(self, documents: List[Dict], score_key: str) -> List[Dict]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–∫–æ—Ä—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]"""
        if not documents:
            return documents
        
        scores = [doc.get(score_key, 0) for doc in documents]
        min_score = min(scores)
        max_score = max(scores)
        
        if max_score == min_score:
            # –í—Å–µ —Å–∫–æ—Ä—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
            for doc in documents:
                doc[f'normalized_{score_key}'] = 1.0
        else:
            for doc in documents:
                original_score = doc.get(score_key, 0)
                normalized = (original_score - min_score) / (max_score - min_score)
                doc[f'normalized_{score_key}'] = normalized
        
        return documents
    
    def _combine_results(self, semantic_results: List[Dict], lexical_results: List[Dict], 
                        alpha: float = 0.7) -> List[Dict]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        alpha: –≤–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0.7 = 70% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π, 30% –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä—ã
        semantic_results = self._normalize_scores(semantic_results, 'semantic_score')
        lexical_results = self._normalize_scores(lexical_results, 'lexical_score')
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        combined_docs = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for doc in semantic_results:
            content_hash = hash(doc['content'])
            combined_docs[content_hash] = {
                **doc,
                'normalized_semantic_score': doc.get('normalized_semantic_score', 0),
                'normalized_lexical_score': 0,
                'sources': ['semantic']
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for doc in lexical_results:
            content_hash = hash(doc['content'])
            if content_hash in combined_docs:
                # –î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –µ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π —Å–∫–æ—Ä
                combined_docs[content_hash]['normalized_lexical_score'] = doc.get('normalized_lexical_score', 0)
                combined_docs[content_hash]['sources'].append('lexical')
            else:
                # –ù–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                combined_docs[content_hash] = {
                    **doc,
                    'normalized_semantic_score': 0,
                    'normalized_lexical_score': doc.get('normalized_lexical_score', 0),
                    'sources': ['lexical']
                }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —Å–∫–æ—Ä
        final_results = []
        for doc in combined_docs.values():
            semantic_score = doc.get('normalized_semantic_score', 0)
            lexical_score = doc.get('normalized_lexical_score', 0)
            
            # –ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–∫–æ—Ä: alpha * semantic + (1-alpha) * lexical
            hybrid_score = alpha * semantic_score + (1 - alpha) * lexical_score
            
            # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –æ–±–æ–∏—Ö —Ç–∏–ø–∞—Ö –ø–æ–∏—Å–∫–∞
            if len(doc['sources']) > 1:
                hybrid_score *= 1.1  # 10% –±–æ–Ω—É—Å
            
            doc['hybrid_score'] = hybrid_score
            doc['relevance_score'] = hybrid_score  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            final_results.append(doc)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É —Å–∫–æ—Ä—É
        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        return final_results
    
    def search(self, query: str, framework: str = None, max_results: int = 5, 
               alpha: float = 0.7) -> List[Dict]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            framework: –§–∏–ª—å—Ç—Ä –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            alpha: –í–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0.7 = 70% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π, 30% –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ —Å–∫–æ—Ä–∞–º–∏
        """
        logger.info(f"üîç HYBRID SEARCH: query='{query}', framework={framework}, alpha={alpha}")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        search_limit = max_results * 2
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_results = self._semantic_search(query, framework, search_limit)
        logger.info(f"üìä SEMANTIC: –Ω–∞–π–¥–µ–Ω–æ {len(semantic_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        lexical_results = self._lexical_search(query, framework, search_limit)
        logger.info(f"üìä LEXICAL: –Ω–∞–π–¥–µ–Ω–æ {len(lexical_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_results = self._combine_results(semantic_results, lexical_results, alpha)
        logger.info(f"üìä COMBINED: –∏—Ç–æ–≥–æ {len(combined_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_results = combined_results[:max_results]
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, doc in enumerate(final_results[:3], 1):
            sources = ', '.join(doc.get('sources', []))
            logger.info(f"üìÑ TOP {i}: hybrid_score={doc['hybrid_score']:.3f}, sources=[{sources}]")
        
        return final_results
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à BM25 –∏–Ω–¥–µ–∫—Å–æ–≤"""
        self.bm25_retrievers.clear()
        self.framework_documents.clear()
        logger.info("üóëÔ∏è –ö—ç—à –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ—á–∏—â–µ–Ω")
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        return {
            'bm25_indexes': list(self.bm25_retrievers.keys()),
            'cached_frameworks': len(self.bm25_retrievers),
            'total_cached_documents': sum(
                len(data.get('documents', [])) 
                for data in self.framework_documents.values()
            )
        }
