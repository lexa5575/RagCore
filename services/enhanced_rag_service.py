#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π RAG —Å–µ—Ä–≤–∏—Å —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
–í–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
"""

import time
import logging
import re
from typing import Dict, List, Optional, Tuple, Any
import requests
import json
import numpy as np

from .rag_service import RAGService, CacheService, ProjectService, FrameworkDetector as BaseFrameworkDetector
from .embedding_cache import EmbeddingCache
from .hybrid_retriever import HybridRetriever
from .framework_detector import FrameworkDetector as EnhancedFrameworkDetector
from models.api_models import QueryRequest, QueryResponse
from utils.key_moment_detector import auto_detect_key_moments
from services.llm import clean_llm_response

logger = logging.getLogger(__name__)

class EnhancedRAGService(RAGService):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π RAG —Å–µ—Ä–≤–∏—Å —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏:
    - –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π + –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    - –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(self, config: Dict[str, Any], collection, embedder, session_manager):
        super().__init__(config, collection, embedder, session_manager)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.embedding_cache = EmbeddingCache(
            cache_dir="./embedding_cache",
            max_size=config.get('cache', {}).get('embedding_cache_size', 10000)
        )
        
        self.hybrid_retriever = HybridRetriever(
            embedder=embedder,
            collection=collection,
            config=config
        )
        
        self.enhanced_framework_detector = EnhancedFrameworkDetector()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'hybrid_searches': 0,
            'framework_detections': 0,
            'avg_response_time': 0.0
        }
        
        logger.info("‚úÖ Enhanced RAG Service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏")
    
    def _create_embedding_with_cache(self, text: str) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç embedding —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_embedding = self.embedding_cache.get_embedding(text)
        if cached_embedding is not None:
            return cached_embedding
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding
        embedding = self.embedder.encode([text])[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.embedding_cache.set_embedding(text, embedding)
        
        return embedding
    
    def query_documents_enhanced(self, question: str, framework: str = None, 
                                max_results: int = 5, use_hybrid: bool = True) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º
        """
        try:
            start_time = time.time()
            
            logger.info(f"üîç ENHANCED QUERY: '{question}' (framework: {framework}, hybrid: {use_hybrid})")
            
            if use_hybrid:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
                documents = self.hybrid_retriever.search(
                    query=question,
                    framework=framework,
                    max_results=max_results,
                    alpha=0.7  # 70% —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π, 30% –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π
                )
                self.performance_stats['hybrid_searches'] += 1
                logger.info(f"üéØ HYBRID SEARCH: Found {len(documents)} documents")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                query_embedding = self._create_embedding_with_cache(question)
                
                where_filter = {}
                if framework:
                    where_filter = {"framework": framework}
                
                results = self.collection.query(
                    query_embeddings=[query_embedding.tolist()],
                    n_results=max_results,
                    where=where_filter if where_filter else None
                )
                
                documents = []
                if results['documents'][0]:
                    for i, (doc, metadata, distance) in enumerate(zip(
                        results['documents'][0], 
                        results['metadatas'][0], 
                        results['distances'][0]
                    )):
                        documents.append({
                            'content': doc,
                            'metadata': metadata,
                            'relevance_score': 1 - distance,
                            'rank': i + 1,
                            'source': 'semantic'
                        })
                
                logger.info(f"üìä SEMANTIC SEARCH: Found {len(documents)} documents")
            
            search_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è SEARCH TIME: {search_time:.3f}s")
            
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå ENHANCED QUERY ERROR: {e}")
            return []
    
    def detect_framework_enhanced(self, question: str, context: str = None, 
                                 project_path: str = None) -> Tuple[Optional[str], float]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ —Å –æ—Ü–µ–Ω–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            detected_framework = self.enhanced_framework_detector.detect_framework_comprehensive(
                question=question,
                file_path=project_path,
                file_content=None,
                context=context
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = 0.0
            if detected_framework:
                confidence = self.enhanced_framework_detector.get_framework_confidence(
                    question, detected_framework
                )
                self.performance_stats['framework_detections'] += 1
                logger.info(f"üéØ FRAMEWORK: {detected_framework} (confidence: {confidence:.2f})")
            
            return detected_framework, confidence
            
        except Exception as e:
            logger.error(f"‚ùå FRAMEWORK DETECTION ERROR: {e}")
            return None, 0.0
    
    def _optimize_llm_parameters(self, question: str, context: str, 
                                num_documents: int) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        """
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = {
            'max_tokens': 800,
            'temperature': 0.2,
            'top_p': 0.8,
            'frequency_penalty': 0.1,
            'presence_penalty': 0.1
        }
        
        question_lower = question.lower()
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞
        if any(word in question_lower for word in ['example', '–ø—Ä–∏–º–µ—Ä', '–∫–∞–∫', 'how']):
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã - –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –º–µ–Ω—å—à–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            params['max_tokens'] = 1000
            params['temperature'] = 0.1
        elif any(word in question_lower for word in ['—á—Ç–æ —Ç–∞–∫–æ–µ', 'what is', 'define']):
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è - —Å—Ä–µ–¥–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            params['max_tokens'] = 600
            params['temperature'] = 0.15
        elif any(word in question_lower for word in ['best practice', '–ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏', 'optimize']):
            # Best practices - –±–æ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            params['max_tokens'] = 1200
            params['temperature'] = 0.25
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if num_documents >= 4:
            params['max_tokens'] += 200  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
        elif num_documents <= 2:
            params['max_tokens'] -= 100  # –ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_length = len(context)
        if context_length > 3000:
            params['max_tokens'] += 150
        elif context_length < 1000:
            params['max_tokens'] -= 50
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        params['max_tokens'] = max(400, min(params['max_tokens'], 1500))
        
        return params
    
    def generate_llm_response_enhanced(self, question: str, context: str, 
                                     framework: str = None) -> str:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        try:
            llm_config = self.config.get('llm', {})
            model_config = llm_config.get('models', {}).get('qwen', {})
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            formatted_context = self._format_context_for_llm(context)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            num_documents = len([part for part in context.split('\n\n') if part.strip()])
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LLM
            optimized_params = self._optimize_llm_parameters(question, context, num_documents)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
            model_config.update(optimized_params)
            
            logger.info(f"ü§ñ LLM PARAMS: max_tokens={optimized_params['max_tokens']}, "
                       f"temperature={optimized_params['temperature']}")
            
            # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            prompt = self._create_enhanced_prompt(question, formatted_context, framework)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å retry —Å–∏—Å—Ç–µ–º–æ–π
            return self._generate_with_retry_enhanced(prompt, model_config)
            
        except Exception as e:
            logger.error(f"‚ùå ENHANCED LLM ERROR: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
    
    def _create_enhanced_prompt(self, question: str, context: str, framework: str = None) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
        """
        framework_instructions = {
            'laravel': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Laravel. –î–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Å PHP –∫–æ–¥–æ–º.",
            'vue': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Vue.js. –ü–æ–∫–∞–∑—ã–≤–∞–π –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ Composition API.",
            'filament': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Filament. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ä–µ—Å—É—Ä—Å–∞—Ö, —Ñ–æ—Ä–º–∞—Ö –∏ —Ç–∞–±–ª–∏—Ü–∞—Ö.",
            'alpine': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Alpine.js. –ü–æ–∫–∞–∑—ã–≤–∞–π –ø—Ä–∏–º–µ—Ä—ã —Å x-data –∏ –¥–∏—Ä–µ–∫—Ç–∏–≤–∞–º–∏.",
            'inertia': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Inertia.js. –û–±—ä—è—Å–Ω—è–π SPA –ø–æ–¥—Ö–æ–¥ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é.",
            'tailwindcss': "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Tailwind CSS. –ü–æ–∫–∞–∑—ã–≤–∞–π utility –∫–ª–∞—Å—Å—ã –∏ –ø—Ä–∏–º–µ—Ä—ã."
        }
        
        system_instruction = framework_instructions.get(framework, 
            "–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç. –î–∞–≤–∞–π —Ç–æ—á–Ω—ã–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã.")
        
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_instruction}

–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
2. –í–∫–ª—é—á–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∫–æ–≥–¥–∞ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —á–µ—Ç–∫–æ –∏ –ª–æ–≥–∏—á–Ω–æ
4. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ –æ–± —ç—Ç–æ–º —Å–∫–∞–∂–∏
5. –ó–∞–≤–µ—Ä—à–∞–π –æ—Ç–≤–µ—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é - –Ω–µ –æ–±—Ä—ã–≤–∞–π –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ

<|eot_id|><|start_header_id|>user<|end_header_id|>

–î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø:
{context}

–í–û–ü–†–û–°: {question}

–î–∞–π –ø–æ–ª–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—ã—à–µ.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        return prompt
    
    def _generate_with_retry_enhanced(self, prompt: str, model_config: dict) -> str:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ retry —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # –î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                current_config = model_config.copy()
                if attempt > 0:
                    current_config['max_tokens'] += attempt * 200
                    current_config['temperature'] = max(0.1, current_config['temperature'] - attempt * 0.05)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                api_url = current_config.get('api_url', 'http://127.0.0.1:1234/v1/completions')
                
                request_data = {
                    "model": current_config.get('model_name', 'meta-llama-3.1-8b-instruct'),
                    "prompt": prompt,
                    "max_tokens": current_config['max_tokens'],
                    "temperature": current_config['temperature'],
                    "top_p": current_config.get('top_p', 0.8),
                    "frequency_penalty": current_config.get('frequency_penalty', 0.1),
                    "presence_penalty": current_config.get('presence_penalty', 0.1),
                    "stream": False
                }
                
                stop_sequences = current_config.get('stop', [])
                if stop_sequences:
                    request_data["stop"] = stop_sequences
                
                response = requests.post(api_url, json=request_data, timeout=60)
                
                if response.status_code == 200:
                    result = response.json()
                    raw_answer = result.get('choices', [{}])[0].get('text', '')
                    
                    # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
                    answer = clean_llm_response(raw_answer)
                    answer = self._post_process_response(answer, "", "")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
                    if not self._is_response_truncated(answer) and len(answer) > 100:
                        logger.info(f"‚úÖ ENHANCED RESPONSE: {len(answer)} chars, attempt: {attempt + 1}")
                        return answer
                    else:
                        logger.warning(f"‚ö†Ô∏è RETRY NEEDED: Attempt {attempt + 1}, quality insufficient")
                        if attempt == max_retries - 1:
                            return answer  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á—Ç–æ –µ—Å—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–µ
                        continue
                else:
                    logger.error(f"‚ùå LLM API ERROR: {response.status_code}")
                    if attempt == max_retries - 1:
                        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"
                    continue
                    
            except Exception as e:
                logger.error(f"‚ùå GENERATION ERROR: {e}")
                if attempt == max_retries - 1:
                    return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
                continue
        
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"
    
    def process_query_enhanced(self, request: QueryRequest) -> QueryResponse:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å –≤—Å–µ–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        """
        start_time = time.time()
        self.performance_stats['total_queries'] += 1
        
        try:
            logger.info(f"üöÄ ENHANCED QUERY START: '{request.question}'")
            
            # 1. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            detected_framework, confidence = self.detect_framework_enhanced(
                question=request.question,
                context=request.context,
                project_path=request.project_path
            )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∏–ª–∏ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫
            final_framework = request.framework or detected_framework
            
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Å —É—á–µ—Ç–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            cache_key = self.cache_service.get_cache_key(request.question, final_framework)
            cached_response = self.cache_service.get_cached_response(cache_key)
            
            if cached_response:
                self.performance_stats['cache_hits'] += 1
                cached_response['response_time'] = time.time() - start_time
                logger.info(f"üíæ CACHE HIT: Response served from cache")
                return QueryResponse(**cached_response)
            
            # 3. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            documents = self.query_documents_enhanced(
                question=request.question,
                framework=final_framework,
                max_results=request.max_results,
                use_hybrid=True
            )
            
            if not documents:
                logger.warning("‚ùå NO DOCUMENTS: No relevant documents found")
                raise Exception("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
            # 4. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = "\n\n".join([doc['content'] for doc in documents])
            if request.context:
                context += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{request.context}"
            
            # 5. –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            answer = self.generate_llm_response_enhanced(
                question=request.question,
                context=context,
                framework=final_framework
            )
            
            # 6. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = []
            for doc in documents:
                source_info = {
                    "title": doc['metadata'].get('section_title', 'Document'),
                    "content": doc['content'][:200] + "..." if len(doc['content']) > 200 else doc['content'],
                    "framework": doc['metadata'].get('framework', 'unknown'),
                    "relevance_score": doc.get('relevance_score', 0.0)
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –ø–æ–∏—Å–∫–∞
                if 'sources' in doc:
                    source_info['search_types'] = doc['sources']
                
                sources.append(source_info)
            
            # 7. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response_time = time.time() - start_time
            
            response_data = {
                "answer": answer,
                "sources": sources,
                "total_docs": len(documents),
                "response_time": response_time,
                "framework_detected": detected_framework,
                "framework_confidence": confidence,
                "session_id": None,
                "session_context_used": False,
                "key_moments_detected": [],
                "performance_info": {
                    "used_hybrid_search": True,
                    "embedding_cache_used": True,
                    "framework_auto_detected": detected_framework is not None
                }
            }
            
            # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self.cache_service.set_cached_response(cache_key, response_data)
            
            # 9. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_performance_stats(response_time)
            
            logger.info(f"üéâ ENHANCED QUERY COMPLETE: {response_time:.3f}s")
            
            return QueryResponse(**response_data)
            
        except Exception as e:
            logger.error(f"‚ùå ENHANCED QUERY ERROR: {e}")
            raise
    
    def _update_performance_stats(self, response_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        total_queries = self.performance_stats['total_queries']
        current_avg = self.performance_stats['avg_response_time']
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
        new_avg = ((current_avg * (total_queries - 1)) + response_time) / total_queries
        self.performance_stats['avg_response_time'] = new_avg
    
    def get_enhanced_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        base_stats = super().get_stats() if hasattr(super(), 'get_stats') else {}
        
        enhanced_stats = {
            **base_stats,
            'performance': self.performance_stats,
            'embedding_cache': self.embedding_cache.get_stats(),
            'hybrid_retriever': self.hybrid_retriever.get_stats(),
            'framework_detector': self.enhanced_framework_detector.get_framework_stats(),
            'cache_hit_rate': (
                self.performance_stats['cache_hits'] / 
                max(self.performance_stats['total_queries'], 1)
            )
        }
        
        return enhanced_stats
    
    def clear_all_caches(self):
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –∫—ç—à–∏"""
        super().cache_service.clear_cache()
        self.embedding_cache.clear_cache()
        self.hybrid_retriever.clear_cache()
        logger.info("üóëÔ∏è –í—Å–µ –∫—ç—à–∏ –æ—á–∏—â–µ–Ω—ã")
    
    def warmup_system(self):
        """–ü—Ä–æ–≥—Ä–µ–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        logger.info("üî• WARMUP: –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º —Å–∏—Å—Ç–µ–º—É...")
        
        # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º embedding –º–æ–¥–µ–ª—å
        warmup_texts = [
            "Laravel model creation",
            "Vue component example", 
            "Filament resource setup",
            "Alpine.js directive usage",
            "Inertia.js routing",
            "Tailwind CSS utilities"
        ]
        
        for text in warmup_texts:
            self._create_embedding_with_cache(text)
        
        # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å—ã
        for framework in ['laravel', 'vue', 'filament', 'alpine', 'inertia', 'tailwindcss']:
            try:
                self.hybrid_retriever._get_or_create_bm25(framework)
            except:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
        
        logger.info("‚úÖ WARMUP: –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
