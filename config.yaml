# 游 RAG Server Configuration
# Universal configuration with relative paths

# 游닄 Database settings
database:
  collection_name: universal_docs
  path: ./chroma_storage

# 游뱄 LLM Configuration (EDIT THIS SECTION!)
# 뉋햄햇먫떓뭻 뤯덇떓 뉋냻먫떓냻뢇 햊 먫냻뫓뛣뤯럟럟돯뢇뭻떓먫햋햑햇

# === 뉋냻먫떓냻뢇 1: LM Studio (햣햨쮏쨿왏쫧얨햣혝혜혪) ===
llm:
  provider: lm_studio
  api_url: http://127.0.0.1:1234/v1/completions
  model_name: meta-llama-3.1-8b-instruct  # 햊행햎햇햏햊햑햇 햏햃 뉋냻뻃 햎뤯덇돯햛
  max_tokens: 800
  temperature: 0.2
  stop: ["<|eot_id|>", "<|end_of_text|>", "Human:", "User:"]

# === 뉋냻먫떓냻뢇 2: Ollama ===
# llm:
#   provider: ollama
#   api_url: http://localhost:11434/api/generate
#   model_name: llama3.1:8b  # 햊행햎햇햏햊햑햇 햏햃 뉋냻뻃 햎뤯덇돯햛 OLLAMA
#   max_tokens: 800
#   temperature: 0.2

# 游깷 Server settings
server:
  host: 0.0.0.0
  port: 8000

# 游늬 Frameworks (auto-updated by update_docs.py)
frameworks:
  # This section is automatically managed
  # Add your documentation to documentation/ folder
  # Run: python3 update_docs.py

# 游늵 Logging
logging:
  level: INFO
  file: ./logs/rag_system.log

# 游 Cache settings
cache:
  enabled: true
  ttl: 3600

# 游댢 Embeddings
embeddings:
  model: all-MiniLM-L6-v2
